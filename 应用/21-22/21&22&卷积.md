## 21-22 clustering & cloud computing

## 集群 clustering

#### 目的

- 增加可扩展性：希望线性（有两倍的机器就有两倍的效率） 但是达不到；
- 可用性：一个崩了，另一个还能用

对用户来说

- 不知道后面是集群，只是给它发请求

正向代理：4个人用同一个ip地址请求出去：

- 例子：vpn

反向代理：大量请求发到一个ip上：负载均衡

- 集群就是一个反向代理：大量请求发到一个IP上，这个IP将请求调度、转发给不同的服务器

在做集群的时候本质就是在做一个反向代理

集群的服务器之间要有一些约束：

- 要满足一定条件服务器才能够被拼起来，去做同步
- 只有同步了才能去做容错

------

#### **RAS 属性**

1. **可靠性（Reliability）：** 消除单点故障。
2. **可用性（Availability）：** 确保总体可用性，计算公式为： 1−(1−𝑓%)^n。
3. **可维护性（Serviceability）：**复杂性比单一服务器更高，但支持热升级。

------

#### load balance 负载均衡策略

负载均衡器根据策略把请求转发到集群的服务器上

- **round-robin 锦标赛：直接轮流发**（对等的转发）
  - 优点：负载 均衡的效果比较好，不同的机器轮流来处理，一般来说比方法三的效果好；
  - 缺点：session需要单 独维护，以及没有方法二的优点)

- **least-connect 看谁当前处理的请求最少就发给谁**

- **ip-hash（根据cookie选择对应的server）**

  - **会话粘滞性**，如果发到了其他机器上会找不到session id

  - 优点：
    - 保证了会话的粘制性，只要用户的IP不变，集群中处理用户请求的机器 就不会变，
  - 缺点：
    - 这是三种方法里面负载均衡效果最差的一种，如果hash函数选择的不好的话，最终的结果很可能是导致集群里面的经常是一台机器在处理请求；
    - 如果用户改变了IP，比如上网途中开启了或者关闭了VPN或者代理服务器，导致用户ip改变，这时候session就可能丢失

- **weight权重：**让服务器资源不同也能获得对应数量的请求

------

#### 容错

- **请求级别的failover**：无状态请求崩了，再调一个（需要幂等性）
- **session级别的failover**：会话之前已经有状态了，需要维护住之前的状态，不用把所有的都重做（如果不维护住就会需要将这个session里的所有请求都重做一次）
  - 状态广播：将session处理的状态广播到服务器上
    - 每台机器上都会有它的会话状态的副本
    - 会造成浪费
  - 用一个统一的session服务器存储各种会话信息
    - 所有其他服务器都无状态；
      - 此时根本不需要ip-hash，因为会话服务器是独立出去的
    - 每个请求都会去这个服务器拿对话id对应的session进行处理，处理完毕后再写回到这个服务器上
    - session服务器单一节点需要备份（不然挂了所有信息都没了）

- **要支持容错：服务器要支持幂等性的操作（失败了可能会多次重做）**
  - GET,PUT,DELETE都是幂等的，只有POST不是幂等的

#### **失败可能点：**

1. 请求发出后，服务方法执行前失败。
   -  解决：换一台服务器。

2. 服务方法执行中途失败。 
   - 解决：重新换服务器，重新拿session执行。

3. 服务方法执行完成后，响应传输失败。
   -  解决：保证系统**严格幂等**的情况下换服务器执行。


------

### Nginx

- 基于HTTP
- 就是一个反向代理，且和后台服务器的类型是解耦的（不要求后台服务器一定是spring boot之类的）
  - **负载均衡：**Nginx可以将传入的请求分发给多个后端服务器，以平衡服务器的负载，提高系统性能 和可靠性。
  - **缓存功能：**Nginx可以**缓存静态文件或动态页面**，减轻服务器的负载，提高响应速度。
  - **动静分离：**将动态生成的内容（如 PHP、Python、Node.js 等）和静态资源（如 HTML、CSS、 JavaScript、图片、视频等）分别存放在不同的服务器或路径上。
  - **多站点代理：**Nginx可以代理多个域名或虚拟主机，将不同的请求转发到不同的后端服务器上，实现多个站点的共享端口。

------

### 数据库集群

![1736692516469](%E9%9B%86%E7%BE%A4nginx.assets/1736692516469-1736936537836-1.jpg)

- 组成：
  - 至少有三个实例

  - 需要一个叫Admin API的工具，使用一系列的命令去配置三个机器

  - **一个类似GateWay的MySQL Router**，负责转发请求。

- 数据库组成可能是一个主MySQL服务器，两个从MySQL服务器，主服务器负责读写操作，后面两个从服务器只能读，然后同步主服务器的内容。
  - 写只能在一台服务器上，会把内容同步到其他服务器上

  - 读请求则在所有服务器上做负载均衡

- MySQL Router，负责转发请求也会保证负载均衡，对于读的操作均衡分配（ 当然主服务器可能会少一点）对于写的操作分配给主服务器。



## Cloud Computing

### 云

云在现实生活中：本质上就是将软件和硬件全部当做服务对外界暴露

##### 好处：

- 不需要去部署一个服务器
- 用多久就付多少钱，可以省钱
- 具有更加灵活的计价机制（短期、长期）
- 可以马上启动
  - 云需要解决快速供给的问题——避免冷启动

##### 特点：

- 虚拟化
  - 物理机上既能模拟出windows、也可以模拟出ubuntu
- 快速供给
- 弹性计价
- 弹性扩展规模
  - 可以很好满足用户需求

##### 云上的核心技术：

- 作业调度

  - google提供的一种方法：map-reduce

- 分布式文件系统

  - distributed google file system
    - chunk server 一台台物理机
    - master server

- Bigtable——分布式文件系统中数据存储的形式

  - 关系型数据库的问题：
    - 如果三张表都很大（三张表存在三个机器中），外键的join操作要在机器之间进行的开销是极大的
    - 竖着切，使用等价类划分，将表的一部分存在A上，另一部分存在B上。
      - 问题：很难切开
  - bigtable解决了这个问题：一个表可以很大很大，可以随便切成若干块处理。且也没有表之间的关联了。
    - 因为少了表之间的关联，所以引入了列组
    - 列组是可以动态变化的，使得bigtable可以存储半结构化的数据
  - 这张表本质上还是一个分布式文件系统的存储的大文件

  ![image-20250115194629073](%E9%9B%86%E7%BE%A4nginx.assets/image-20250115194629073.png)

  - 如果这张表实在太大也可以做切分，表达成上述形式之后表就不会太大了，也就不用再进一步切分了

- 云操作系统的对应汇总：（google）

  - FS：GFS
  - job：mapreduce
  - DB：bigtable
  - 内存管理：没有统一架构
    - gaussdb的内存管理就是把它变成了一个分布式缓存
  - **hadoop提供了上面这些内容的一个开源实现**
    - hdfs
    - mr——yarn
    - hbase——bigtable
    - hive



#### 云原生

开发、部署、编译、一直到最后的运行全部都在云上

主要特征

- 云上的所有东西都是微服务，而且都以serverless的模式在运行
- 所有东西都是以容器化的模式运行
- 和本地的开发是有一些差异的



### 边缘计算

相比云计算，虽然最后一层也是发到云中，但有些计算会在边缘服务器中直接完成并返回，不需要每次都进入云中，让计算在靠近数据来源的地方进行

- 可以节省带宽
- 且设备一般不会一直与云相连但是会一直与边缘设备相连，速度也更快

#### **概念以及产生背景**

##### 概念

- 边缘计算是在**靠近数据来源**（如 IoT 设备、手机）的地方进行数据处理和存储，而不是完全依赖云端。

- 边缘服务器：
  - 负责接收设备上传的数据。
  - 对本地数据进行预处理，并定期上传核心云端。

##### 产生背景

- 原来所有操作都是在云里面做处理

  - 如果要访问总要将请求发到云端，会有很大的时延，也会对云端压力太大，**想要节省带宽**

- 但是有大量边缘设备：

  - 比如**基站**（含有边缘服务器）
  - 在网络的边缘，服务器具有一定的存储、计算的能力
  - 有些请求边缘服务器能计算就不发到云中，而是直接在边缘服务器中处理完返回
  - 将一些计算节点下放到边缘设备，加快速度




#### cloud offloading

**cloud offloading：操作在设备之间的迁移**

原则：边缘能处理就处理，太多就往上一级发

- 有些计算到底是在边缘里做还是在云里做呢？
  - **边缘还分为微基站、宏基站**
- 如果**本来决定微基站处理，但是大量请求涌入使得微基站处理不了了，就会给到宏基站，宏基站也处理不了则会进一步给到云端**
  - 例子：摄像头处理视频，摄像头内置cpu，自己先处理，白天人太多就截图发到上一级
- 能够处理该计算的服务器构成一个树形结构
  - 到底会在哪个层次做处理会由节点的状态动态决定
  - 这个计算会在不同的节点之间做迁移




#### The Rise of Cloud-Terminal Fusion Computing 云边段融合的崛起

- Cloud Servers 云服务器
- Mobile Edge Computing Servers 边缘服务器
- Mobile Devices (Edge Devices) 移动设备，即端侧

- 云边端融合（云 边缘 端侧）是现在的整体趋势
  - 边缘设备搞不定再往上发
  - 对端侧设备透明，它不会知道计算最后在哪里进行，只会知道计算被完成了




#### **挑战**

- 数据存储和处理的分布式特性：
  - 如何快速定位存储在边缘还是云端的数据。
    - 如果query发到核心，既要在核心云里找也要在边缘找，**要怎么知道去哪里找呢？**
  - 查询需要在多个地方（边缘和云）协调。

------

#### **边缘存储**

##### 边缘存储的特性

1. **流式数据**：数据连续生成和传输。
2. **数据量大**：需要高效的存储和压缩。
3. **范围查询**：支持对特定范围的数据进行快速查询。

![1736692073320](%E9%9B%86%E7%BE%A4nginx.assets/1736692073320.jpg)

- #### **解决方案**

  - 元数据快速同步至云端：提供对数据位置的快速索引。
  - 详细数据保留在边缘存储：减少核心云端的存储压力。

------

![1736692418846](%E9%9B%86%E7%BE%A4nginx.assets/1736692418846.jpg)

![1736692427676](%E9%9B%86%E7%BE%A4nginx.assets/1736692427676.jpg)

- 计算下推：节省网络传输开销；云中心计算减少；充分利用节点计算能力



## 神经网络

keras搭建的时候，搭建出来的是一个前端，后端一定要是tensorflow、pytorch和另一个三者之一。

#### 训练原理：

将神经元的结果汇总后发给几个激活函数进行处理

![image-20250115190005047](%E9%9B%86%E7%BE%A4nginx.assets/image-20250115190005047.png)

- 输入一般是带有权重的
- 将所有输入加在一起后就得到了净输入

![image-20250115190111523](%E9%9B%86%E7%BE%A4nginx.assets/image-20250115190111523.png)

- sigmoid函数：
  - 任意函数可以变成若干个阶跃函数的线性组合，但是很难去描述这种组合
  - 他就是想要模拟这个阶跃函数，并且以单一的函数标识
- 主要记住：希望有一堆这样的函数，能够通过它们的组合模拟出一个接近真实的情况——激活函数



##### 为什么需要很多神经元：

![image-20250115190322478](%E9%9B%86%E7%BE%A4nginx.assets/image-20250115190322478.png)

![image-20250115191012778](%E9%9B%86%E7%BE%A4nginx.assets/image-20250115191012778.png)

- 单个神经元无法描述复杂函数
- 需要多个神经元串在一起，用线性组合的方式得到对复杂函数逼近的一个结果
- 上图中：
  - 第一个灰色的是输入层，输入可能是多维的，输入之前需要先摊平。

![image-20250115191425094](%E9%9B%86%E7%BE%A4nginx.assets/image-20250115191425094.png)

神经网络的输出使用的是**“独热编码”**

- 输出层里面只有一个1，其他全是0——理想情况

- 所以要分多少个类，输入层就要有几层



#### 神经网络的结构

如果从输入的角度和从输出的角度都看不见——**隐藏层**

- 线越多，能够识别的特征越多。线越少，能够识别的特征越少
- 如果神经元太多可能过拟合。如果神经元太少可能欠拟合

![image-20250115191710287](%E9%9B%86%E7%BE%A4nginx.assets/image-20250115191710287.png)

- 可以将其看作一种复杂的计算引擎
- 使用训练数据来训练它
- 期待它能够对新的数据起到不错的效果

![image-20250115191751448](%E9%9B%86%E7%BE%A4nginx.assets/image-20250115191751448.png)

- 每一层的输出都会作为下一层的输入的一部分
- 权重就代表了机器学习到的结果

- 权重的意义：

  - 红色表达这个值对最后的判断有利
  - 蓝色表示这个值对最后判断没用
  - 权重算出来的东西具有一定的普适性

  ![image-20250115193350290](%E9%9B%86%E7%BE%A4nginx.assets/image-20250115193350290.png)

![1736698701235](%E9%9B%86%E7%BE%A4nginx.assets/1736698701235.jpg)

![1736698741246](%E9%9B%86%E7%BE%A4nginx.assets/1736698741246.jpg)

##### softmax函数就是sigmoid对多类别的扩展：

![image-20250115192253532](%E9%9B%86%E7%BE%A4nginx.assets/image-20250115192253532.png)

- 经过这个计算，1、2、3中最大的占比会被放大，只要占的不是最大的就会被压小
  - 所以希望将预测对的结果放大——softmax函数
- softmax另一个作用：和损失函数结合去计算梯度的降低

激活函数一定是非线性的

- 因为n个线性函数也等价于一个1个，设置n个神经元层就没有了意义

训练的时候会随机遮住一部分特征让模型进行识别——dropout函数做的就是这个

- 防止模型是根据某个局部的特征去进行识别的

#### 全连接神经网络

（Fully Connected Neural Network, FCNN）

**全连接层**的定义：就是这一层的所有神经元与上一层的所有神经元都相连——Dense就代表是全连接层

- 下一层能收到上一层所有的输出，某些边不会没有，称为是dense的

**多层感知机**（Multilayer Perceptron, MLP）

- **每个神经元与上一层的所有神经元相连**，并通过**权重和偏置**调整连接强度

![1736698777432](%E9%9B%86%E7%BE%A4nginx.assets/1736698777432.jpg)

#### **全连接网络的局限性**：

- 全连接网络对输入数据的每个分量是“平等对待”的，没有考虑数据的内在结构关系。
- 对于图像数据，这种方法会导致需要学习的权重参数过多（例如，一个200×200的RGB彩色图像需要至少14亿个权重参数）。
- 全连接网络无法充分利用图像的**空间信息**（如像素之间的局部关系）。

![1736698790680](%E9%9B%86%E7%BE%A4nginx.assets/1736698790680.jpg)

------



### 卷积神经网络

（Convolutional Neural Network, CNN）

![image-20250115200050543](%E9%9B%86%E7%BE%A4nginx.assets/image-20250115200050543.png)

- 输入变量彼此之间没有关系，但是对于图来说不是这样的

![image-20250115200114857](%E9%9B%86%E7%BE%A4nginx.assets/image-20250115200114857.png)

- 图片会有非常多的参数，如果全部是全连接，会发现权重数量太多，所以会使用卷积神经网络

![image-20250115200205901](%E9%9B%86%E7%BE%A4nginx.assets/image-20250115200205901.png)

- 上面是一个卷积网络的运行例子：
  - 先看猫有没有一双眼睛
  - 一双眼睛是不是有两个圆圈，里面各套着一个圆圈，两者之间的距离是否正常
  - 再进一步看这个眼睛能不能和周围的东西构成一张猫的脸
- 即看一个局部一个局部，再将一些局部放到一起看他们是不是构成一个更大的局部
- 先提取局部特征，再将局部特征拼到一起看它是不是一个完整的分类（是不是一只猫）
- 且特征提取工具可以复用

#### **卷积核（Kernel）**

![image-20250115200436037](%E9%9B%86%E7%BE%A4nginx.assets/image-20250115200436037.png)

- 覆盖的位置乘以权重的值，得到居中像素的值

- 计算的具体例子：

  ![image-20250115200537578](%E9%9B%86%E7%BE%A4nginx.assets/image-20250115200537578.png)

  - 将卷积核矩阵套在input矩阵上，对应位置做计算，算出来之后将特征值填在居中像素

- **卷积核**是一个小的滤波器（数字信号处理中），用于扫描图像并提取特征，**也就是一个局部特征提取器**

- 叫卷积的原因：

  - 这个概念来自财经界，算净收益由来


#### 卷积的结构

- 用一个能套取方块特征的卷积核套上去卷
- 再用一个能套取圆特征的卷积核套上去卷
  - 此时认为得到了一个channel，通道
- 基础类型有圆、方形等等
- 这里面的所有特征合起来能否合成一个更复杂的图形：
  - 比如一个发动机的途径
  - 此时就是将前一步的所有channel放在一起去卷，用一个更大的卷积核

#### **卷积操作**

- 卷积核在图像上滑动（通过设定步幅 Stride），对每一块区域执行加权求和操作，生成特征图（Feature Map）。
- 通过调整卷积核的大小、深度和步幅，可以控制输出特征图的尺寸和深度。
  - 缩小图的大小——称为池化，避免使用的卷积核太大

![1736698811796](%E9%9B%86%E7%BE%A4nginx.assets/1736698811796.jpg)

![1736698827696](%E9%9B%86%E7%BE%A4nginx.assets/1736698827696.jpg)

#### 其他特点：

- 卷积核的行数和列数一般都是奇数

- 为了使卷积的结果和输入的一样大，会有填充

- 步幅可以不断变化

- 卷积核的深度与输入通道的数量相同

- 卷积深度的相关知识：

  ![image-20250115201541644](%E9%9B%86%E7%BE%A4nginx.assets/image-20250115201541644.png)

- 卷积神经网络可以做叠加

  - 卷积层可以不断添加，能够提升准确率，但是运行也会更慢

#### **卷积神经网络的优点**

- **减少参数数量**：通过共享卷积核，大大减少了需要学习的参数，降低了计算成本。
- **空间不变性**：通过卷积操作，模型能很好地处理目标的平移和尺度变化。
- **局部感知能力**：卷积核可以捕获图像的局部特征（如边缘、纹理等）。



