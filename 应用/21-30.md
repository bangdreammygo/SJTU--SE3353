# 应用体系架构（大三上） SE十二童首(11-20)

------

# 21-22 clustering & cloud computing

## 集群 clustering

#### 目的

- 增加可扩展性：希望线性（有两倍的机器就有两倍的效率） 但是达不到；
- 可用性：一个崩了，另一个还能用

对用户来说

- 不知道后面是集群，只是给它发请求

正向代理：4个人用同一个ip地址请求出去：

- 例子：vpn

反向代理：大量请求发到一个ip上：负载均衡

- 集群就是一个反向代理：大量请求发到一个IP上，这个IP将请求调度、转发给不同的服务器

在做集群的时候本质就是在做一个反向代理

集群的服务器之间要有一些约束：

- 要满足一定条件服务器才能够被拼起来，去做同步
- 只有同步了才能去做容错

------

#### **RAS 属性**

1. **可靠性（Reliability）：** 消除单点故障。
2. **可用性（Availability）：** 确保总体可用性，计算公式为： 1−(1−𝑓%)^n。
3. **可维护性（Serviceability）：**复杂性比单一服务器更高，但支持热升级。

------

#### load balance 负载均衡策略

负载均衡器根据策略把请求转发到集群的服务器上

- **round-robin 锦标赛：直接轮流发**（对等的转发）
  - 优点：负载 均衡的效果比较好，不同的机器轮流来处理，一般来说比方法三的效果好；
  - 缺点：session需要单 独维护，以及没有方法二的优点)

- **least-connect 看谁当前处理的请求最少就发给谁**

- **ip-hash（根据cookie选择对应的server）**

  - **会话粘滞性**，如果发到了其他机器上会找不到session id

  - 优点：
    - 保证了会话的粘制性，只要用户的IP不变，集群中处理用户请求的机器 就不会变，
  - 缺点：
    - 这是三种方法里面负载均衡效果最差的一种，如果hash函数选择的不好的话，最终的结果很可能是导致集群里面的经常是一台机器在处理请求；
    - 如果用户改变了IP，比如上网途中开启了或者关闭了VPN或者代理服务器，导致用户ip改变，这时候session就可能丢失

- **weight权重：**让服务器资源不同也能获得对应数量的请求

------

#### 容错

- **请求级别的failover**：无状态请求崩了，再调一个（需要幂等性）
- **session级别的failover**：会话之前已经有状态了，需要维护住之前的状态，不用把所有的都重做（如果不维护住就会需要将这个session里的所有请求都重做一次）
  - 状态广播：将session处理的状态广播到服务器上
    - 每台机器上都会有它的会话状态的副本
    - 会造成浪费
  - 用一个统一的session服务器存储各种会话信息
    - 所有其他服务器都无状态；
      - 此时根本不需要ip-hash，因为会话服务器是独立出去的
    - 每个请求都会去这个服务器拿对话id对应的session进行处理，处理完毕后再写回到这个服务器上
    - session服务器单一节点需要备份（不然挂了所有信息都没了）

- **要支持容错：服务器要支持幂等性的操作（失败了可能会多次重做）**
  - GET,PUT,DELETE都是幂等的，只有POST不是幂等的

#### **失败可能点：**

1. 请求发出后，服务方法执行前失败。
   -  解决：换一台服务器。

2. 服务方法执行中途失败。 
   - 解决：重新换服务器，重新拿session执行。

3. 服务方法执行完成后，响应传输失败。
   -  解决：保证系统**严格幂等**的情况下换服务器执行。


------

### Nginx

- 基于HTTP
- 就是一个反向代理，且和后台服务器的类型是解耦的（不要求后台服务器一定是spring boot之类的）
  - **负载均衡：**Nginx可以将传入的请求分发给多个后端服务器，以平衡服务器的负载，提高系统性能 和可靠性。
  - **缓存功能：**Nginx可以**缓存静态文件或动态页面**，减轻服务器的负载，提高响应速度。
  - **动静分离：**将动态生成的内容（如 PHP、Python、Node.js 等）和静态资源（如 HTML、CSS、 JavaScript、图片、视频等）分别存放在不同的服务器或路径上。
  - **多站点代理：**Nginx可以代理多个域名或虚拟主机，将不同的请求转发到不同的后端服务器上，实现多个站点的共享端口。

------

### 数据库集群

![1736692516469](C:/Users/77043/Desktop/应用/21-22/集群nginx.assets/1736692516469-1736936537836-1.jpg)

- 组成：
  - 至少有三个实例

  - 需要一个叫Admin API的工具，使用一系列的命令去配置三个机器

  - **一个类似GateWay的MySQL Router**，负责转发请求。

- 数据库组成可能是一个主MySQL服务器，两个从MySQL服务器，主服务器负责读写操作，后面两个从服务器只能读，然后同步主服务器的内容。
  - 写只能在一台服务器上，会把内容同步到其他服务器上

  - 读请求则在所有服务器上做负载均衡

- MySQL Router，负责转发请求也会保证负载均衡，对于读的操作均衡分配（ 当然主服务器可能会少一点）对于写的操作分配给主服务器。



## Cloud Computing

### 云

云在现实生活中：本质上就是将软件和硬件全部当做服务对外界暴露

##### 好处：

- 不需要去部署一个服务器
- 用多久就付多少钱，可以省钱
- 具有更加灵活的计价机制（短期、长期）
- 可以马上启动
  - 云需要解决快速供给的问题——避免冷启动

##### 特点：

- 虚拟化
  - 物理机上既能模拟出windows、也可以模拟出ubuntu
- 快速供给
- 弹性计价
- 弹性扩展规模
  - 可以很好满足用户需求

##### 云上的核心技术：

- 作业调度

  - google提供的一种方法：map-reduce

- 分布式文件系统

  - distributed google file system
    - chunk server 一台台物理机
    - master server

- Bigtable——分布式文件系统中数据存储的形式

  - 关系型数据库的问题：
    - 如果三张表都很大（三张表存在三个机器中），外键的join操作要在机器之间进行的开销是极大的
    - 竖着切，使用等价类划分，将表的一部分存在A上，另一部分存在B上。
      - 问题：很难切开
  - bigtable解决了这个问题：一个表可以很大很大，可以随便切成若干块处理。且也没有表之间的关联了。
    - 因为少了表之间的关联，所以引入了列组
    - 列组是可以动态变化的，使得bigtable可以存储半结构化的数据
  - 这张表本质上还是一个分布式文件系统的存储的大文件

  ![image-20250115194629073](C:/Users/77043/Desktop/应用/21-22/集群nginx.assets/image-20250115194629073.png)

  - 如果这张表实在太大也可以做切分，表达成上述形式之后表就不会太大了，也就不用再进一步切分了

- 云操作系统的对应汇总：（google）

  - FS：GFS
  - job：mapreduce
  - DB：bigtable
  - 内存管理：没有统一架构
    - gaussdb的内存管理就是把它变成了一个分布式缓存
  - **hadoop提供了上面这些内容的一个开源实现**
    - hdfs
    - mr——yarn
    - hbase——bigtable
    - hive



#### 云原生

开发、部署、编译、一直到最后的运行全部都在云上

主要特征

- 云上的所有东西都是微服务，而且都以serverless的模式在运行
- 所有东西都是以容器化的模式运行
- 和本地的开发是有一些差异的



### 边缘计算

相比云计算，虽然最后一层也是发到云中，但有些计算会在边缘服务器中直接完成并返回，不需要每次都进入云中，让计算在靠近数据来源的地方进行

- 可以节省带宽
- 且设备一般不会一直与云相连但是会一直与边缘设备相连，速度也更快

#### **概念以及产生背景**

##### 概念

- 边缘计算是在**靠近数据来源**（如 IoT 设备、手机）的地方进行数据处理和存储，而不是完全依赖云端。

- 边缘服务器：
  - 负责接收设备上传的数据。
  - 对本地数据进行预处理，并定期上传核心云端。

##### 产生背景

- 原来所有操作都是在云里面做处理

  - 如果要访问总要将请求发到云端，会有很大的时延，也会对云端压力太大，**想要节省带宽**

- 但是有大量边缘设备：

  - 比如**基站**（含有边缘服务器）
  - 在网络的边缘，服务器具有一定的存储、计算的能力
  - 有些请求边缘服务器能计算就不发到云中，而是直接在边缘服务器中处理完返回
  - 将一些计算节点下放到边缘设备，加快速度




#### cloud offloading

**cloud offloading：操作在设备之间的迁移**

原则：边缘能处理就处理，太多就往上一级发

- 有些计算到底是在边缘里做还是在云里做呢？
  - **边缘还分为微基站、宏基站**
- 如果**本来决定微基站处理，但是大量请求涌入使得微基站处理不了了，就会给到宏基站，宏基站也处理不了则会进一步给到云端**
  - 例子：摄像头处理视频，摄像头内置cpu，自己先处理，白天人太多就截图发到上一级
- 能够处理该计算的服务器构成一个树形结构
  - 到底会在哪个层次做处理会由节点的状态动态决定
  - 这个计算会在不同的节点之间做迁移




#### The Rise of Cloud-Terminal Fusion Computing 云边段融合的崛起

- Cloud Servers 云服务器
- Mobile Edge Computing Servers 边缘服务器
- Mobile Devices (Edge Devices) 移动设备，即端侧

- 云边端融合（云 边缘 端侧）是现在的整体趋势
  - 边缘设备搞不定再往上发
  - 对端侧设备透明，它不会知道计算最后在哪里进行，只会知道计算被完成了




#### **挑战**

- 数据存储和处理的分布式特性：
  - 如何快速定位存储在边缘还是云端的数据。
    - 如果query发到核心，既要在核心云里找也要在边缘找，**要怎么知道去哪里找呢？**
  - 查询需要在多个地方（边缘和云）协调。

------

#### **边缘存储**

##### 边缘存储的特性

1. **流式数据**：数据连续生成和传输。
2. **数据量大**：需要高效的存储和压缩。
3. **范围查询**：支持对特定范围的数据进行快速查询。

![1736692073320](C:/Users/77043/Desktop/应用/21-22/集群nginx.assets/1736692073320.jpg)

- #### **解决方案**

  - 元数据快速同步至云端：提供对数据位置的快速索引。
  - 详细数据保留在边缘存储：减少核心云端的存储压力。

------

![1736692418846](C:/Users/77043/Desktop/应用/21-22/集群nginx.assets/1736692418846.jpg)

![1736692427676](C:/Users/77043/Desktop/应用/21-22/集群nginx.assets/1736692427676.jpg)

- 计算下推：节省网络传输开销；云中心计算减少；充分利用节点计算能力



## 神经网络

keras搭建的时候，搭建出来的是一个前端，后端一定要是tensorflow、pytorch和另一个三者之一。

#### 训练原理：

将神经元的结果汇总后发给几个激活函数进行处理

![image-20250115190005047](C:/Users/77043/Desktop/应用/21-22/集群nginx.assets/image-20250115190005047.png)

- 输入一般是带有权重的
- 将所有输入加在一起后就得到了净输入

![image-20250115190111523](C:/Users/77043/Desktop/应用/21-22/集群nginx.assets/image-20250115190111523.png)

- sigmoid函数：
  - 任意函数可以变成若干个阶跃函数的线性组合，但是很难去描述这种组合
  - 他就是想要模拟这个阶跃函数，并且以单一的函数标识
- 主要记住：希望有一堆这样的函数，能够通过它们的组合模拟出一个接近真实的情况——激活函数



##### 为什么需要很多神经元：

![image-20250115190322478](C:/Users/77043/Desktop/应用/21-22/集群nginx.assets/image-20250115190322478.png)

![image-20250115191012778](C:/Users/77043/Desktop/应用/21-22/集群nginx.assets/image-20250115191012778.png)

- 单个神经元无法描述复杂函数
- 需要多个神经元串在一起，用线性组合的方式得到对复杂函数逼近的一个结果
- 上图中：
  - 第一个灰色的是输入层，输入可能是多维的，输入之前需要先摊平。

![image-20250115191425094](C:/Users/77043/Desktop/应用/21-22/集群nginx.assets/image-20250115191425094.png)

神经网络的输出使用的是**“独热编码”**

- 输出层里面只有一个1，其他全是0——理想情况

- 所以要分多少个类，输入层就要有几层



#### 神经网络的结构

如果从输入的角度和从输出的角度都看不见——**隐藏层**

- 线越多，能够识别的特征越多。线越少，能够识别的特征越少
- 如果神经元太多可能过拟合。如果神经元太少可能欠拟合

![image-20250115191710287](C:/Users/77043/Desktop/应用/21-22/集群nginx.assets/image-20250115191710287.png)

- 可以将其看作一种复杂的计算引擎
- 使用训练数据来训练它
- 期待它能够对新的数据起到不错的效果

![image-20250115191751448](C:/Users/77043/Desktop/应用/21-22/集群nginx.assets/image-20250115191751448.png)

- 每一层的输出都会作为下一层的输入的一部分

- 权重就代表了机器学习到的结果

- 权重的意义：

  - 红色表达这个值对最后的判断有利
  - 蓝色表示这个值对最后判断没用
  - 权重算出来的东西具有一定的普适性

  ![image-20250115193350290](C:/Users/77043/Desktop/应用/21-22/集群nginx.assets/image-20250115193350290.png)

![1736698701235](C:/Users/77043/Desktop/应用/21-22/集群nginx.assets/1736698701235.jpg)

![1736698741246](C:/Users/77043/Desktop/应用/21-22/集群nginx.assets/1736698741246.jpg)

##### softmax函数就是sigmoid对多类别的扩展：

![image-20250115192253532](C:/Users/77043/Desktop/应用/21-22/集群nginx.assets/image-20250115192253532.png)

- 经过这个计算，1、2、3中最大的占比会被放大，只要占的不是最大的就会被压小
  - 所以希望将预测对的结果放大——softmax函数
- softmax另一个作用：和损失函数结合去计算梯度的降低

激活函数一定是非线性的

- 因为n个线性函数也等价于一个1个，设置n个神经元层就没有了意义

训练的时候会随机遮住一部分特征让模型进行识别——dropout函数做的就是这个

- 防止模型是根据某个局部的特征去进行识别的

#### 全连接神经网络

（Fully Connected Neural Network, FCNN）

**全连接层**的定义：就是这一层的所有神经元与上一层的所有神经元都相连——Dense就代表是全连接层

- 下一层能收到上一层所有的输出，某些边不会没有，称为是dense的

**多层感知机**（Multilayer Perceptron, MLP）

- **每个神经元与上一层的所有神经元相连**，并通过**权重和偏置**调整连接强度

![1736698777432](C:/Users/77043/Desktop/应用/21-22/集群nginx.assets/1736698777432.jpg)

#### **全连接网络的局限性**：

- 全连接网络对输入数据的每个分量是“平等对待”的，没有考虑数据的内在结构关系。
- 对于图像数据，这种方法会导致需要学习的权重参数过多（例如，一个200×200的RGB彩色图像需要至少14亿个权重参数）。
- 全连接网络无法充分利用图像的**空间信息**（如像素之间的局部关系）。

![1736698790680](C:/Users/77043/Desktop/应用/21-22/集群nginx.assets/1736698790680.jpg)

------



### 卷积神经网络

（Convolutional Neural Network, CNN）

![image-20250115200050543](C:/Users/77043/Desktop/应用/21-22/集群nginx.assets/image-20250115200050543.png)

- 输入变量彼此之间没有关系，但是对于图来说不是这样的

![image-20250115200114857](C:/Users/77043/Desktop/应用/21-22/集群nginx.assets/image-20250115200114857.png)

- 图片会有非常多的参数，如果全部是全连接，会发现权重数量太多，所以会使用卷积神经网络

![image-20250115200205901](C:/Users/77043/Desktop/应用/21-22/集群nginx.assets/image-20250115200205901.png)

- 上面是一个卷积网络的运行例子：
  - 先看猫有没有一双眼睛
  - 一双眼睛是不是有两个圆圈，里面各套着一个圆圈，两者之间的距离是否正常
  - 再进一步看这个眼睛能不能和周围的东西构成一张猫的脸
- 即看一个局部一个局部，再将一些局部放到一起看他们是不是构成一个更大的局部
- 先提取局部特征，再将局部特征拼到一起看它是不是一个完整的分类（是不是一只猫）
- 且特征提取工具可以复用

#### **卷积核（Kernel）**

![image-20250115200436037](C:/Users/77043/Desktop/应用/21-22/集群nginx.assets/image-20250115200436037.png)

- 覆盖的位置乘以权重的值，得到居中像素的值

- 计算的具体例子：

  ![image-20250115200537578](C:/Users/77043/Desktop/应用/21-22/集群nginx.assets/image-20250115200537578.png)

  - 将卷积核矩阵套在input矩阵上，对应位置做计算，算出来之后将特征值填在居中像素

- **卷积核**是一个小的滤波器（数字信号处理中），用于扫描图像并提取特征，**也就是一个局部特征提取器**

- 叫卷积的原因：

  - 这个概念来自财经界，算净收益由来


#### 卷积的结构

- 用一个能套取方块特征的卷积核套上去卷
- 再用一个能套取圆特征的卷积核套上去卷
  - 此时认为得到了一个channel，通道
- 基础类型有圆、方形等等
- 这里面的所有特征合起来能否合成一个更复杂的图形：
  - 比如一个发动机的途径
  - 此时就是将前一步的所有channel放在一起去卷，用一个更大的卷积核

#### **卷积操作**

- 卷积核在图像上滑动（通过设定步幅 Stride），对每一块区域执行加权求和操作，生成特征图（Feature Map）。
- 通过调整卷积核的大小、深度和步幅，可以控制输出特征图的尺寸和深度。
  - 缩小图的大小——称为池化，避免使用的卷积核太大

![1736698811796](C:/Users/77043/Desktop/应用/21-22/集群nginx.assets/1736698811796.jpg)

![1736698827696](C:/Users/77043/Desktop/应用/21-22/集群nginx.assets/1736698827696.jpg)

#### 其他特点：

- 卷积核的行数和列数一般都是奇数

- 为了使卷积的结果和输入的一样大，会有填充

- 步幅可以不断变化

- 卷积核的深度与输入通道的数量相同

- 卷积深度的相关知识：

  ![image-20250115201541644](C:/Users/77043/Desktop/应用/21-22/集群nginx.assets/image-20250115201541644.png)

- 卷积神经网络可以做叠加

  - 卷积层可以不断添加，能够提升准确率，但是运行也会更慢

#### **卷积神经网络的优点**

- **减少参数数量**：通过共享卷积核，大大减少了需要学习的参数，降低了计算成本。
- **空间不变性**：通过卷积操作，模型能很好地处理目标的平移和尺度变化。
- **局部感知能力**：卷积核可以捕获图像的局部特征（如边缘、纹理等）。







------

# 23.-Vitualization & Container & CNN神经网络

## 容器

### 容器的设计原则（不重要）

容器本身是有一个设计的原则的--容器调研的PPT，总的来说容器是用Linux里面Cgroup2做的性能隔离。容器本身超过课程的范围，
来讲一下容器是怎么开发的。

###  容器是什么

        容器是什么，刚刚提到的就是用Linux中的Cgroups和文件系统中的namespaces命名规则，做的一个运行环境的隔离。

在运行的时候我们需要一个Runnable的image的实例，这个image就是对整个运行环境做了一个打包。

Image里面就包含了我们所有运行的一个环境。（A Running container uses an isolated filesystem）Cgroup之间就隔离开了，就要保证
A的容器和B的容器他俩的空间要完全隔离开的，我对A的任何操作，就不应该影响对B的任何操作。本质上这个隔离就是靠Cgroups实现的。

容器里有什么？ 其实就是所有运行我们环境所需要的依赖（dependencies),Configurations，binaries。所以容器之间有大小差异。

        Docker使用GO语言写的。总的概念就是我们用namespace去给所有的容器加一个前缀，然后Cgroup语句把容器彼此之间隔离开。

![img_1.png](C:/Users/77043/Desktop/应用/23-24/23/img_1.png)

### 23.1.3 容器和虚拟机的区别

虚拟机内部是一个完整的OS，但是在容器里面，它实际上实在一个Linux的一个宿主机上面跑一个个的容器。这些容器在运行的时候，我们要考虑容器它是
分层的。 

        容器分层的目的是什么呢？比如我们开一个Springboot的项目，然后别人开发一个C#的项目，这些项目可能要依赖于底下一些共同的部分。
    于是这些共同的部分可以专门打包成一个Layer层。所以这些容器就可以变成若干层累积起来了。所以Docker内部是一个分层的结构。

所以我们科学上网，pull一个镜像的时候，我们可以看到镜像是一层层下载的，考虑分层的目的就是要做复用。就是有很多相同的东西
，不同的容器可以共享一层，这一层的代码就不用反复去拉取了，提高效率。

      虚拟机和容器最大的区别就是虚拟机有一个独立的操作系统。

这个完整独立的操作系统就可以告诉你，假如使用Windows的环境里面运行一个
Linux环境的虚拟机。但是Docker就会想，既然都是Linux操作系统，那我们为什么不直接基于这个操作系统，然后彼此之间的环境隔离开就可以了。
    

        所以Docker其实是要跑在一个类Linux的操作环境里头。

![img.png](C:/Users/77043/Desktop/应用/23-24/23/img.png)

### Docker的架构

我们在client端主要命令就是比如docker build可以本地构建一个镜像，就可以把他推到一个注册表（DockerHub）里去。

我们也可以使用docker pull拉下来一个镜像，我们可以在镜像里面去实例化一个容器。

docker run就是去实际跑起来一个容器。

![img_2.png](C:/Users/77043/Desktop/应用/23-24/23/img_2.png)

### 23.1.5 DockerFile

DockerFile在告诉你，这个Docker镜像里面有些什么东西，就比如我们在docker build的时候，就会读这个DockerFile的内容，这个内容就告诉我们
怎么去打包这个image。

### Container Volumn（数据持久化）

我们是否可以在容器启动的过程中不断动态添加卷（Volumn）上去，这些卷可以存我们想要的东西。但就像我们做作业中遇到的问题，我们直接给容器外挂
一个卷，重启容器后，这个卷内的数据就会丢失，怎么持久化这个卷里面的数据？

    我们需要先建立一个卷，我们再次运行容器的时候，除了对端口进行映射之外我们还需要对卷进行映射，对这个容器外挂这个卷。因为将容器关了以后，我们的卷
    还在，重启容器的时候还会把这个卷挂载进去，数据就不会丢失。


再进一步，如果我们不想要把数据搞成一个卷，挂载到容器上，我想自己管理文件，放到自己电脑的文件夹里面，该怎么办？

    那我们就要使用Bind Mounts，Bind Mounts和Named Volumn的区别就是，Named Volumn的位置是由Docker来决定的，而Bind Mounts的
    位置我们可以自行决定。

### 23.1.7 多个容器间通信

多个容器间通信，首先我们需要让多个容器跑在同一个网络内部。

    容器只有在同一个网络中，彼此间才能通信。

所以我们得首先使用docker network create指令创建一个网络。然后在这个网络中运行后续这多个容器。

### 23.1.8 Docker Compose

对于多个容器，我们能不能把这些容器一次性全部提来，这就需要Docker compose，我们要使用.yaml文件，里面包含所有Services的名称，image名字，端口映射
，过载卷，镜像环境...

    写好docker——compose.yml后我们就可以通过docker compose up来一键提起所有的容器。

## 23.2 CNN 自然语言处理基础知识

### 23.2.1 自然语言处理简介

自然语言处理涉及到以下几个方面：

![img_3.png](C:/Users/77043/Desktop/应用/23-24/23/img_3.png)

怎么把自然语言数值化：

1. word->vector：（NLP研究1领域2 ~ 应用3领域2），缺点：字符种类过多，我们要用到的数值就过多了。而且各单词间有关联，我们得把单词变成Vector，如果
   简单的把每个单词和其他的单词的关联写成[0,0,..,1,...1,..0,0]类似的Vector，若有100000个单词、则维度也为100000，显然过高了。
2. BOW（词袋模型）：对于有哪些词语，我直接把它放到一个袋子中（Bag of Word),然后按照它们的顺序进行排序。

卷积: 多个相连单词间的关系

![img_4.png](C:/Users/77043/Desktop/应用/23-24/23/img_4.png)


    CNN可以用来做自然语言处理，我多设几个卷积层、池化层，第一层卷积层检测多个相邻单词间的关系，第二层检测多个相邻词组间的关系，
    以此类推，慢慢的就可以变成一个完整的句子。

### 23.2.2 自然语言处理示例

首先我们要知道怎么分词，把文本数据所有单词分开——可以使用字典（都是一个一个的单词，包含权重和词性）。然后我们需要获得语料库（corpus），即我们用于训练的
文本数据，（课中的例子是许多的新闻合集），对这个语料进行分词预处理。对于分词后的结果我们对之进行数据清洗，（课中的例子过滤了极值，这里过滤了只出现一次的词，以及
所有语料中50%以上都出现的值）。构建BOW模型，把之前分词后的结果构建为词袋模型（词袋模型具体做什么——拿着字典到你的语句里面找，每个字符都做了多少次，他做了一个统计，
统计完之后他按照这些单词的ID的顺序排列）。然后用N—Gram构建Phrases模型，对词组进行处理。再后面做了一个主题分类，使用GenSim在BOW模型基础上构建LDA模型，
查看每个文本被分到不同主题的概率。

CNN怎么运用的。
![img_5.png](C:/Users/77043/Desktop/应用/23-24/23/img_5.png)
    

    顺序模型（普通CNN）：我们在对文本进行训练时，我们kernel_size为n，则找n个单词间的关系。然后用MaxPool进行池化，再卷积，就可以进而得到n个词组间的关系，
    所以效果很好。
    
    优化模型（TextCNN）：在卷积和池化层后，我们对池化层1、池化层2、池化层n的结果进行了一个拼接，相当于我们把短，中，长句子的特征都拼接到了一起，
    然后拉平，dropout一下，准确率会大大提升。而且如果各句子彼此长度差别大，该模型效果一定更好。
    
    ChatGPT用的是RNN循环神经网络，RNN的结果会更好！RNN不仅可以做语言处理、还可以做文本生成。





------

# 24-Hadoop & RNN模型

## Hadoop

### Hadoop简介

Hadoop是Apache的项目，主要就是提供一个分布式的计算环境，就包含了云操作系统里面的一些实现，具体就是要解答分布式计算环境里面怎么让大量机器
看起来就像单个服务器一样做计算，实际上是在做大集群计算。

Hadoop项目包含了以下模块：Hadoop Common——支持其他Hadoop模块的common utilities，
Hadoop Distributed File System（HDFS）——一个高并发处理数据的分布式文件系统，Hadoop YARN，Hadoop MapReduce——分布式作业调度；Hadoop
Ozone——数据存储（对象存储）等等

HDFS实际上不是一个从底端开始，需要格式化才能装的文件系统，实际上是在类Linux操作系统上加了一层。所以在Linux或者类Linux（比如Mac上可以直接跑这个程序），
HDFS内部一个file可以分成若干tablet以及block，这些block被真正存到文件系统中。为了提高可靠性，在分布式环境中，我们一般要加一个副本。

![img.png](C:/Users/77043/Desktop/应用/23-24/24/img.png)

整个文件分区表存在namenode中，客户端每个请求先要进入namenode中，client->namenode->HDFS（block)->Linux FileSystem。所以打开hadoop首先要启动
namenode。

### Map Reduce

Map: 如果一个A映射到B，如果A的长度是10，那么B的长度也是10，Map就是做一个集合映射。

Reduce： 把集合变成一个值，相当于一个规约。![img_1.png](C:/Users/77043/Desktop/应用/23-24/24/img_1.png)


具体：Word Count例子：我每次Map完，对于Map的结果可以先在本地做一个Combine，然后把Combine的结果写到一个Reduce的worker可以读到的地方。然后我再调用一个Reduce，对combine结果进行规约
这才是真正的规约得到的结果。
![img_7.png](C:/Users/77043/Desktop/应用/23-24/24/img_7.png)

经过Combine操作后：
![img_8.png](C:/Users/77043/Desktop/应用/23-24/24/img_8.png)

Shuffle是什么：我们规定每个Reducer处理对应段的字符串，因此在Map后我们要Shuffle，方便每个Reducer读取自己需要处理的集合（比如把A-N放一起，O-Z放一起），
Shuffle同时我们还要对结果进行sort，两个是一体的。
![img_10.png](C:/Users/77043/Desktop/应用/23-24/24/img_10.png)
![img_2.png](C:/Users/77043/Desktop/应用/23-24/24/img_2.png)
![img_3.png](C:/Users/77043/Desktop/应用/23-24/24/img_3.png)
![img_4.png](C:/Users/77043/Desktop/应用/23-24/24/img_4.png)
![img_5.png](C:/Users/77043/Desktop/应用/23-24/24/img_5.png)

### input数据格式

Map阶段的输入是raw NCDC data（原始NCDC数据），我们首先需要按行取出，在处理是时候我们在前面加入一个偏置量，知道每条数据的长度。数据map-reduce具体流程：

![img_6.png](C:/Users/77043/Desktop/应用/23-24/24/img_6.png)

### Combiner

Combine 可以用也可以没有：

比如我们在求最大值时，我们可以在每个Mapper后做Combine操作，（其实就是对数据进行预处理）有些情况可以节省效率，有些情况不能优化。

    比如求平均值，我们对每个节点的map结果进行平均值计算的预处理就是没有任何意义的，所以具体情况具体分析。

### input splits

Hadoop会把传给Map Reduce的出入数据切分成固定大小的pieces，称之为input splits，对于每个split调用一个map task。
split就是HDFS的block，默认值是64MB。

### JobTracker V1.0

Map reduce中任务调度是怎么控制的呢。我们有一个jobTracker。我们先讨论JobTracker1.0的情况。

图中黄颜色的是我们的代码，我们提交一个作业给jobclient，发一个client包给hadoop，hadoop就把包给JobTracker，JobTracker就会新建一个job，然后就有可能
会读取一些HDFS上面的文件，创建好job后面就会找一些执行程序的机器，JobTracker会把代码扔给这些机器，执行的过程中会起一个单独的进程（TaskTracker），在这个过程
中TaskTracker一直给JobTracker发心跳包，如果心跳停止就寻找新的节点。

心跳：如10分钟内必须来三次，（一个时间段内来n次）。没有限制得很死。

    缺点：JobTracker需要管理所有可用资源、还要跟踪监控job运行状态，监控task tracker，任务过重。
    JobTracker成为系统的瓶颈。

![img_9.png](C:/Users/77043/Desktop/应用/23-24/24/img_9.png)

### 多少个Reducer

0.95到1.75倍的节点数乘以每个节点能开的容器数。如果认为节点处理速度不强就用0.95，
如果处理速度很强，就用1.75。

![img_11.png](C:/Users/77043/Desktop/应用/23-24/24/img_11.png)

### Partitioner

如果存在若干个reducer，我们可以使用Partitioner对数据进行分区，（可以设置）。

### Hadoop的缺陷

我们的数据都在磁盘中，会有大量的I/O操作，是一个重度I/O的操作，极大影响效率，我们是否可以把数据放入内存中——这就引出了下一节课的Spark。


![img_12.png](C:/Users/77043/Desktop/应用/23-24/24/img_12.png)

- 如果我一个节点的map操作中途出现问题，前面的处理都作废了，那么我们可以如何优化他？我们可以在每次map得到中间文件后直接Shuffle&Sort，传给Reducer节点，

![img_13.png](C:/Users/77043/Desktop/应用/23-24/24/img_13.png)

### Map Reduce 2.0（YARN)

 - 原来version1的jobTracker里面实际上是把资源管理和作业的跟踪，是合在一起的，于是呢就会觉得瓶颈就会出现，Hadoop里面就会有一个所谓的下一代的
   Map Reduce，也就是Yarn。这里面就把两个职责给分开了，资源管理由Resource Manager（Global全局的）管理，作业跟踪就用Application Master管理，而且不是有单独一个
   Application Master来跟踪所有的节点，而是由每一个节点都有自己的Application Master。应用就是一堆的作业，彼此之间是一个DAG。

![img_14.png](C:/Users/77043/Desktop/应用/23-24/24/img_14.png)

每一个Node内部都有一个Node Manager，Node Manager就会把node的所有信息发给Resource Manager，所以当客户端在发给Resource Manager
一个请求的时候，他就可以Node Manager提供的信息找最适合的节点去处理作业。

跟踪这个任务作业实时状态的逻辑和容器生成与销毁的逻辑都放在App Mstr里面去完成，
如果他发现一个容器心跳没了，就会像Resource Manager申请一个新的节点去retry。

通过这个分离职责，任务追踪和资源管理就解耦了。实现了更加均衡的实现方式。

## RNN循环网络神经

### 循环神经网络含义

我们有一个网络层，我们的输入进去之后，产生了这一层的输出ht，当我输入下一个输入的时候，对于这个神经元来说，（比如这个输入是x0，输出也是h0），当输入x1的时候，
h0也会当成一个输入输回网络层。也就是说，每一次输出都包含了前面的输出提取出来的特征，就非常适合自然语言处理以及连续性数据的预测。（比如股票市场，我要预测一天的
股价，我们可以把前面20天股价数据输入进来，这样20天里面所有的特征全部用来预测最终的结果）我们要限定一个我们要回看多少行。

- 缺点：程序不能并行，必须要串行，速度比较慢。

![img_15.png](C:/Users/77043/Desktop/应用/23-24/24/img_15.png)

### LSTM

 我们碰到不同长度的句子全部都用一个RNN模型吗，如果有的序列特别长的时候，效率显然不太好。所以我们可不可以分情况，可以把某些输出扔掉不进行保留，不继续传下去呢？

为了提高灵活性，我们需要引入LSTM模型。

![img_16.png](C:/Users/77043/Desktop/应用/23-24/24/img_16.png)

LSTM内部有许多的门，我们输入值进入以后，需要经过一个sigmod函数（激活函数），这样就会有一个0到1的范围。
如果我们的输入值经过激活后接近于0，跟前面输出的h(t-1)相乘，就相当于我把h(t-1)忘掉了，如果接近1，h(t-1)就会传下去。然后我们将h(t-1)和x(t)相加，
作为这一层的结果传下去。这样不会将所有前面的输出特征传下去。

具体如何结合h(t-1)和xt逻辑的算式如下，比较逆天，看不懂。

![img_17.png](C:/Users/77043/Desktop/应用/23-24/24/img_17.png)

![img_18.png](C:/Users/77043/Desktop/应用/23-24/24/img_18.png)







------

# 25-spark

- 对大规模数据做分析，在内存中进行，比较快速
- 支持的语言很多：JAVA | PYTHON | SCALA
- Spark本身用Scala写的， 用Java代码量是其6倍

## Spark 的主要构件

 1. ### **批处理 / 流式数据：Batch / Streaming data**

    - Batch data 和之前看到的 `Hadoop` 里面做 `MapReduce`是一样的

    - Streaming data 始终是源源不断的来，没有开始没有结束，因此与 Batch 的处理不一样

      

 2. ### **支持 SQL 的操作**

    - 底层的数据实际上不是结构化数据的表，不是关系型数据库的表

    - 实际数据是以 `RDD` 或者 `DataFrame` 的形式

      > 把文件读进来实际上就是读成 **DataFrame**

    - 在 `DataFrame` 之上可以用 **SQL** 的方式去操作它

      > Spark 能够把 **SQL** 语句转化成对 **DataFrame** 的操作
      >
      > 因此不需要在 **DataFrame** 上记录很多专有的接口，只需要用 **SQL** 就能直接操作它

 3. ### **支持机器学习和数据分析**

    - 目前用 `Pytorch` 或者 `Tensorflow` ，因此只是简单介绍一下

## Spark 优点

1. 性能表现很好

   - 使用内存计算，workloads 比 `Hadoop` 快了 100 多倍

     > Hadoop 在 MapReduce 等操作时重度使用磁盘 IO

   - 数据结构更简单

     > 使用内存，内存是一维的、线性的，因此数据结构可以比Hadoop更简单，比如一维一直去增长

   - **因为Spark是完全内存计算，因此对内存的需求比较大，如果内存开的太小，就会进行虚拟内存和磁盘间大量的换页操作，从而导致速度的降低**

2. Spark 拥有多用途的核心组件

   1. **Spark Core**
      - 支持让Spark跑起来，包括 `RDD` 这样的数据集，以及支持并行
   2. **Spark SQL**
   3. **Spark Streaming**
   4. **MLlib**（机器学习的库）
   5. **GraphX**
      - 专门对图数据进行加速处理，包括图计算、图上的机器学习

## Spark 实例：Log Mining

### 场景：有一个Driver（Master）和三个Worker，以及要读入一个规模很大的Log，需要过滤出其中报错的消息 (以下为SCALA代码)

1. `lines = spark.textFile("hdfs://...")`

​       **使用Spark的上下文环境，将 HDFS 中的某个文本文件读取进来**

- 这里要注意的是 lines 读进来的是 `RDD` （分布式数据集）在内存里， 是不可以改的

- 也就是说数据集被 Spark 加载进来后，本身的值就不能被修改了，意味着可以复用（例如不用担心多线程问题，在集群环境下就可以免除竞争、加锁的情况）

  

2. `errors = lines.filter(_.startsWith("ERROR"))`

   **类似Map的转换（transformation）操作，从一个 `RDD` 变成另一个 `RDD`，即过滤 lines 里面以 ERROR 开头的信息**

- 这里的 errors 转换并不会改变 lines 里的内容



3. `messages = errors.map(_.split('\t')(2))`

   **将 ERROR 信息中的部件分隔开，并取出其中的第二个部件，也即错误日志的messages，这是第三个 `RDD`**

   

4. `cachedMsgs = messages.cache()`

   **将messages给cache住**

- **为什么要将其cache住？**
  - 内存是有限的，把所有`RDD`放在内存里，那内存满了怎么办？此时有两种方法：
    - 一种是使用 `LRU` 把一些不使用的RDD直接抹除（Remove）
    - 另一种是使用 `SWAP` 把最远被访问的写到disk，把新进来的写到内存
  - 此时的 `cache` 就相当于在内存中做了一次  `PERSIST` ，此时上述两种操作就不会作用到这个 `RDD` 
  - 但是这也只是一个愿望，如果内存最后实在不够，还是会进行 `SWAP` 等操作
- **Spark是分 stage 执行的**
  - 在前三行代码执行过程中，Spark只是维护一张计算图，并没有进行任何的计算操作
  - 即使加载进来的文件不存在，如果没有运行第四行的cache，也不会报错
  - 只有进行到cache操作（跨stage），才会反推回去把前三行执行，此时才会报错加载文件不存在等错误
  - cache住的RDD是要持久化在内存中，而前三个RDD是为了产生cache的RDD产生的中间结果，此时就可以进行LRU或者SWAP，这是可以接受的



5. `cachedMsgs.filter(_.contains("foo")).count` 

   **把包含`foo`的拿出来，`count` 相当于 Reduce 操作，同样前半部分是一个 transformation，仍然不会立刻执行，而到后半部分的count （Action） 才会执行**

- 具体的计算（例如这里的 count）是要并行执行的，日志分部分加载在三个worker上，三个worker同时进行count，最后汇总
- 当然由于这里的RDD是被Cache住的，因此可以复用来计数其他内容，比如 bar ...



> **总流程 ：**
>
> **-> Log 被分成三个 Block 加载到三个 Worker上  （注意：从lines加载到后面的所有RDD是在三个Worker上都有的，逻辑上是一个，但实际是三个）**
>
> **-> Driver 派发任务** 
>
> **-> Worker 并行处理，将结果返回给 Driver**
>
> **-> Driver 进行汇总**
>
> 
>
> 具体怎么分成三块，以及并行计算后如何汇总，这就是Spark框架帮我们做好的了
>
> 总结来说：Transformation、Action、分stage执行、遇到stage边界后反向回推执行、RDD不可修改、RDD是分布式对象
>
> 除了第一步涉及磁盘读，其他所有都是内存操作（理论上）

## Spark 运行

### 至少要有两个节点：一个 Cluster Manager，以及至少一个的 Worker Node

应用程序通过一个spark context 提交任务给 Manager

Manager 会把任务派发给 Worker node 执行，执行过程中 Manager 要监控 Worker Node， 任务执行过程中频繁使用Worker Node所在机器上的 cache

最终结果会返回给 应用程序



由于集群的运行被 Cluster Manager 屏蔽掉了，因此在外部看来就是在多线程上运行的应用程序

- Worker Node 需要可以被 Manager 网络寻址 （network addressable）
- Manager 上要跑某一种集成管理工具，例如 YARN / Mesos / Kubernetes / Zookeeper （Manager 内置）
- Manager 运行方式分为 Standalone / Apache Mesos / Hadoop YARN /  Kubernetes
- Manager 同时也负责 监控 和 任务调度

**Spark 提交执行脚本：spark-submit script**

- (spark-submit 所在路径)/spark-submit --class "运行的程序主类名" --master (master启动的URL) target/(打好的jar包)

> **流程总结：**
>
> 1. **写好一个应用，打包成 jar 包，丢给 script 脚本 （驱动程序）**
>
>    > 注意直接运行应用会报错 `找不到主类`，必须打 jar 包丢给 script 脚本
>    >
>    > SCALA 也是jar包， PYTHON 是 py 脚本
>
> 2. **通过 submit script 交给 Cluster Manager**
>
> 3. **Cluster Manager 找到 可用的 Worker Node 进行 任务分发 、 调度、 监控**
>
> 4. **Worker Node 中的 Executor 执行 任务 （ Job），通过一堆的 Task 执行得到，并且执行过程中要写一个DAG，分stage执行**

![Spark 运行逻辑](C:/Users/77043/Desktop/应用/25-26/spark'/img/1.png)

**Spark 还支持 SCALA 版本 和 PYTHON 版本的 命令行 shell， 分别为 spark-shell 和 pyspark，这里的具体操作可能不重要，因此我在笔记最后和三种版本的Spark 代码一起贴个图**

## Spark RDDs 弹性分布式数据集

### Resilient Distrubuted Dataset

- 弹性：可以很大也可以很小
- 分布式：逻辑上是一个对象，但可能分布在多个节点上，最大的特点是在多个节点存一个数据集可以并行处理
- 如何获得一个RDD：
  - 调用 （SparkContext的`parallelize`方法）`sc.parallelize`就可以把一个数组等变成一个RDD
  - 调用 （SparkContext的`textFile`方法）`sc.textFile`就可以加载一个文件成为一个RDD

1. **RDD 一般都是分区的 （Partition），可以 load 到一个 集群里面，一旦 load 到一个 集群，将来运行一个任务时会对每个partition都有一个task**

   

2. **RDD有 Transformation 和 Action 两种操作**

   - Transformation : 将一个 dataset 变成 另一个 dataset （典型：Map）

     - **所有 Transformation 都是 Lazy 的，不会立刻执行操作，而是等一个 stage 结束 或者 遇到一个 cache，才会反推执行所有内容**

     - 包括 **map 操作**：对 RDD 中 所有的内容 做一个 映射，比如平方...

       ​	 **flatmap 操作**：类似 机器学习中的 flatten 层，在做摊平操作（比如对["hello world", "hi"] 做 flatmap 会得到一个一维数组，包括 hello、world、hi 三个词

       ​	 **filter 操作**：做筛选，比如选出所有的”error“

       ​	 **union 操作**：做合并，比如将 errorsRDD.union(warningsRDD) 得到一个 badlinesRDD

     - 以上操作**均不会改变RDD，只会创建新的**

   - Action：对 dataset 进行计算 并 返回一个值 给 driver program（典型：Reduce）

     - 包括 **reduce 操作**：对 RDD 中的内容 做给定操作，比如求和
       **count 操作**：做行数的计算

       ​         **collect 操作**：统计 dataset 中所以元素并返回一个数组

   - **为什么要把操作做上述两类区分？**

     - 计算是分 stage 的，在 stage 内部是一个 lazy 的计算方式，stage 告诉我们什么时候开始计算

3. **Spark 中触发事件的操作 ：Shuffle （类似 Hadoop）**

![Shuffle](C:/Users/77043/Desktop/应用/25-26/spark'/img/3.png)

![Spark计算基本流程](C:/Users/77043/Desktop/应用/25-26/spark'/img/2.png)

## Spark 分 Stage 计算

> **假设程序在内存中持有一个非常大的用户信息表（UserID，UserInfo），其中UserInfo包含用户订阅的主题列表**
>
> **有一个很小的表，是过去五分钟内用户在网页上点击链接的事件（UserID，LinkInfo）**
>
> 这两张表看起来是通过UserID关联的表，但实际上是两个RDD，不是关系型的表
>
> **程序周期性地对两张表做`合并`和`查询`**

1. ### **No Partition**

   ![No Partition](C:/Users/77043/Desktop/应用/25-26/spark'/img/4.png)

   - 如果不做分区，用户数据只是按照加入进来的顺序进行切分，那么数据是没有什么规律的，因此若要查询用户A的点击事件，需要在events集合中全都查找一遍才能生成join操作中的一块数据
   - 如果要生成两张表的join结果，就需要用userData中每个数据找一遍events中的所有数据，**效率很低**

2. ### **Partition**

   **在加载（UserID，UserInfo）时，使用哈希进行分区**

   ![partition](C:/Users/77043/Desktop/应用/25-26/spark'/img/5.png)

   - 和之前的差别在于，不做分区时，同一个用户信息可能分布在不同批次加载进来的数据中，因此join需要遍历userData
   - 而采用了 Partition 后同一个用户就只用到一个块中找，**join子分区只依赖于一个userData父分区**，得到了一个**性能优化**

## Dependency 依赖的类型

- **窄依赖（Narrow Dependency）**
  - **父分区的每个RDD只被子分区的一个RDD使用**（注意不要反）
  - 上面的 join 和 userData
  - **而且如果在分布式环境下，同一个窄依赖可以放在同一台机器，从而提升性能**
- **宽依赖（Wide Dependency）**
  - **父RDD的每个分区都可能被多个子RDD分区所使用**（一个父指向多个子）
  - 上面的 join 和 events
  - 性能差

![dependency](C:/Users/77043/Desktop/应用/25-26/spark'/img/6.png)

​										**上图中左侧是 窄依赖 ， 右侧是 宽依赖**



## 窄依赖 和 宽依赖 对比

​	要注意的是**宽依赖是不可避免的，但是我们应该尽可能使用窄依赖**![vs](C:/Users/77043/Desktop/应用/25-26/spark'/img/7.png)



## Stage

**将整个计算画出一个DAG图，将其中窄依赖尽可能放在一个stage**

![stage](C:/Users/77043/Desktop/应用/25-26/spark'/img/8.png)

**以C开始为例：C->D/E->F均没有跨stage，因此当执行到G时，stage2结束，才会回推执行CDEF**

- **好处**
  - 只要执行一个操作，如果是 Tran：
    1. 一定要创建一个新RDD，如果stage很长，那**在很早的时候就计算出RDD**就会**占用、浪费内存**，而当跨stage后，不得不用前面的RDD去计算了，再让前面的RDD去计算
    2. 并且 stage 内部是窄依赖，一个分区只依赖一个分区，并且几个分区可能在同一台机器的内存中，速度很快，因此**在结束时才计算**就可以**节省内存占用**

- 上图中**黑色部分是进行cache**，**为什么E中有个分区进行了 cache 但是没有单独分一个 stage**？
  - 因为之前没有用到RDD，也就是说这个cache实际上是被直接拿来复用的，不是依赖于RDD生成的cache操作



## RDD Cache（Persist）

**RDD Cache 分为以下几种内存存储逻辑**

- **MEMORY_ONLY**：始终放在 内存，如果放不下会把其他RDD移除或者放进DISK，等到用到再重新计算或者拿回来
- **MEMORY_AND_DISK**：可以放在内存里，不适合放内存的时候可以把一部分分区拿到DISK
- **MEMORY_ONLY_SER** (Java and Scala)：只放在内存，并且RDD对象是一个序列化之后的对象，将来可以用于在不同服务器之间传输
- **MEMORY_AND_DISK_SER** (Java and Scala)：跟上面的对应
- **DISK_ONLY**：只放在DISK
- **MEMORY_ONLY_2, MEMORY_AND_DISK_2**, etc. 和上面一样，只是要把每个分区放在两个node做备份
- **OFF_HEAP** (experimental)

**当数据要被移除时通常用 LRU， 也可以用 unpersist 做一个反向操作来处理掉**

## Spark SQL

**底层放在一个RDD 或者 DataFrame 中， 可以用 SQL 语句来操作**

- RDD 是一个完整的对象，比如一个person就有name、info等等
- DataFrame 是一个结构化更好的数据对象，有列式结构，结构化更好（也是创建好就不能修改了）

**如果对 SQL 更熟悉的用户，使用Spark也很方便**

## Structured Streaming 流式数据处理

**数据源源不断的来，如果存入一个表中，这个表就是在无限增长，如何进行word count类似的操作呢**

- 微观上仍然是 Batch 批处理
  - 增量式处理，某个时间点对距离上一次处理之间新增的输入 / 对原来输入的更改 进行处理和返回，处理结果会一直保留
  - 总体来看是流式处理，微观上看是批处理（增量式）

**数据可以通过 kafka 等工具传入**

- 数据传进来后会被切成一个个小批量的批数据来传入Spark Engine，时间间隔很短，每个批数据的处理是增量式的而不是孤立的，看起来就是一直在流式处理

**在数据湖中提到过的流批一体在Spark也可以做**

## Spark 中的 MLlib 和 GraphX

​	重点在于做并行，虽然现在大多使用pytorch等张量计算，但是对于非张量处理（随机森林、决策树、逻辑回归）可以用MLlib

​	GraphX就是将节点和边分别存在两个不同的RDD中，这样就可以将图做的很大



# 26 - Storm(基于流数据处理框架)

## Storm 框架

- 数据源（Spout）就像水龙头，不断输入数据，可以来自队列或者数据库
- 中间有一些处理节点（Bolt），对流入数据进行处理，解析图片、文本、统计字数等
- 处理完后交给下一个处理节点进行处理，处理在微观上仍然是批处理（类似spark）

![框架](C:/Users/77043/Desktop/应用/25-26/storm/img/1.png)

输入源+处理结构，是一个**计算的DAG图**，也叫一个**计算拓扑**

## Storm 运行逻辑

**和Spark、Hadoop非常类似**

- 默认使用**Zookeeper**进行集群管理
- 由 **Nimbus** 作为管理节点来管理一堆 **worker**
- 每个**Worker**上有一个 **SuperVisor**，作为监听器监听 Worker 上的内容和状态并汇总到 **Nimbus**
- 一个 **Worker** 中分了许多 **Slots**，一个 Slot 可以理解为单独执行一个任务的一个进程，受 **SuperVisor** 监控
- **Nimbus** 根据 **SuperVisor** 的汇总进行 **Worker** 的统一调度

![运行架构](C:/Users/77043/Desktop/应用/25-26/storm/img/2.png)

## Storm 集群中的组件

**与Hadoop 集群很像**

- 与 Hadoop 不同的是，Storm不是运行 `MapReduce` 这样输入给好的作业job，而是运行 `topologies`
- `topologies` 和 `job` 在逻辑上很相似，但是区别在于 job 最终会 finish，而 topologies 是永远没有停止的

**同样需要一个 master 与至少一个 Worker Node**

- master 节点上运行 **Nimbus** 进程，与 Hadoop 的 **JobTracker** 类似，负责在集群中分发代码、将任务分配给机器以及监控故障
- 每个 worker node 上运行一个 **SuperVisor** 进程，负责监听机器的工作状态，以及接受 Nimbus 命令来启动或者停止机器运行
- 每个 worker node 上运行 **topologies** 的一个**子集**，topologies 通过这些 worker 来执行和传播
- 所有 Nimbus 和 Supervisor 之间的联系通过 **Zookeeper 集群**实现（机器挂了等等...)

## Storm 中的 Topologies

**一张 DAG 计算图**

其中包含处理节点 bolt 与 源节点 spout，这些节点共同构成了 stream 流

执行方式类似 spark，也是使用脚本来跑

`storm jar all-my-code.jar org.apache.storm.MyTopology arg1 arg2 `

- 这将运行带有参数 arg1 和 arg2 的类 org.apache.storm.MyTopology
- 该类的 main 函数定义拓扑并将其提交给 Nimbus。
- storm jar 部分负责连接到 Nimbus 并上传 jar

### Data Model

Storm 使用 `tuples` 作为数据模型，即 a named list of values

**下面是一个Bolt 的实例**

![](C:/Users/77043/Desktop/应用/25-26/storm/img/3.png)

**以及多个 Bolt 如何 结成一个 Topologies**

![](C:/Users/77043/Desktop/应用/25-26/storm/img/4.png)

其中构建了一个 包含 1个 spout 和 2个 bolt 的 topologies，其中的 10、3、2是传给处理函数的参数

**最终运行效果类似 Spark 的宽依赖执行：**

![](C:/Users/77043/Desktop/应用/25-26/storm/img/5.png)

## Zookeeper

**前面一直讲的 Zookeeper 到底是啥**

- 监控集群中的所有机器
- 会在所有机器中选一个 leader，通过心跳来管理其他节点





------

# ChatGPT

实际上是很简单的文本生成模型——**每一次只生成一个词**

在给定一个输入文本的场景之下，它会预测下一个单词出现概率最高的是哪一个

- **如果一直选概率最高的，只要输入一样不就会千篇一律的回答吗？**

  - **Chatgpt 中有一个`temperature` 的参数，也即温度**
  - 温度是 0 的时候， 就选概率最大的那个词
  - 温度不是 0 的时候，就有一定概率选择概率不是那么高的词
  - **这样由于每次都是选一个单词加入内容，然后基于现有内容再决策出下一个单词，当可以选择概率不是那么高的单词时，就可以基于不同的内容进行生成，具有很高的灵活性，体现了`explore` 和 `exploit`，即探索和利用之间的平衡** 
  - 只探索就会全随机，甚至最后不成话；只利用就会得到千篇一律的答案

- **下一个词是根据概率来选，那这个概率从哪里来？**

  - 概率是从语料中来的，语料则是从已有的资源中来

  - 比如统计 Wiki 中 cat 页面 和 dog 页面中出现的字母，当碰到 cat 或者 dog 时，会根据这个统计结果进行预测

  - 根据统计结果生成一个新的单词，就会根据字母出现的概率生成一个字符串，通过限制单词长度或者统计时额外加上空格，就可以生成一个比较像单词的结果（长度比较像英语单词了）

    ![](C:/Users/77043/Desktop/应用/25-26/chatGpt/img/1.png)

- **最后生成出来还是不像一个单词？如何优化？**

  - 采用 `2-gram` 双词源概率，即选字母的时候不是看单个字母的概率，而是看选了一个字母后它的后面出现的字母的概率

    ![](C:/Users/77043/Desktop/应用/25-26/chatGpt/img/2.png)

  - 左图是单个词源的概率，右图是双词源概率，每一单元格代表：**列字母后跟着行字母的概率**

  - 比如热图中 **q** 这一列几乎只有 **u** 这里有概率，说明 q 后面 几乎都是跟着 u 才能形成单词的

  - 在生成单词时，根据当前字母后面跟着的字母概率，以及温度（用来调控随机性）来选择下一个字母

  - 当然最后就演化成3词源、4词源，一直到 n词源，到了 n词源后就已经能生成很像样的单词了

  - **ChatGPT 是 把字母推广到了词，即一个词后面跟着的下一个词的概率是什么**

- **ChatGPT 把 字母 推广到 词 遇到的困难？**

  - 使用 `n-gram` n词源逻辑可能能够生成比较好的语句，但是问题在于字母只有26个，**而词是非常多的**
  - 因此采用降维的方法——Embedding
  - 不用很高的维度来描述单词，降低维度后，一些特征比较相近的就会落在一起



> **总结基本逻辑：**
>
> 1. **一定要先通过 Embedding 降维，不仅要 Embedding 单词 还要 Embedding 它在句子中的位置，两者加和才是这个单词的特征**
> 2. **经过 n 多个 注意力机制（a sequence of attention blocks），也即 transformer 的 main events，注意力就是两个单词之间的关系是什么样的，得到最终的嵌入特征**
> 3. **最终这些数值将作为全连接以及分类输出的依据**







------

# 27-HDFS

- 来源：谷歌写的一个分布式文件系统的实现。
  **特征：**
- **专门用来存大文件的,至少大到需要分成不少block，小文件别用hdfs**
- 流式数据访问,在写入时**最好只写入一次(并行写存在问题,最好有个写入器写)**(读可以多次),**数据不适合做修改**!
- 廉价硬件:经常是使用一堆廉价硬件来实现的.

**不合适:**

- 数据不大的时候不适合.
- 对数据读写有低延迟要求时也不适合.(hdfs的写入效率和读入效率都不算高)
- 如果想多人写入/修改数据的时候不适合.

**适合:**

- 如视频这类是可以的,存电影这类是可以的,不修改,也很大.

## hdfs的架构:

![](C:/Users/77043/Desktop/应用/27-28/1.png)
上层节点进行元数据处理(比如怎么切block,切出来的block放在哪里之类的),底下的节点是实际存储数据的节点.
用户过来会先到上面去操作元数据,namenode在metadata里找到相关信息后返回给client的jar包,拿到信息后拿着客户端去底下读数据,namenode只操作元数据,实际数据和namenode无关.

### **副本情况:**

副本主要两种,同一个基架下的副本和不同基架下的副本.----这导致读操作的时候可以负载均衡,而写操作需要对全部副本写入.(流水作业,因此浪费时间).

### **启动与心跳**

启动集群先起namenode再启datanode.通过datanode发心跳给namenode来确认datanode.(确认活着进架构,确认副本规则不被破坏),没收到心跳就认为死了,把数据发给其他节点.这里的心跳中还含有bolck-report:报告自身的data情况,帮助namenode把控block数据的情况.
整体还是**master-slave架构**,namenode是master,datanode是slave.
一般来说namenode会专门拿台机器跑,和datanode分开.datanode一个机器上可以跑一个或者多个.(但实际上跑多个没啥意义,跑一个一般,但也不排斥就是了)

### **一些小点**

namenode里设置的副本数量不是一个全局的超参数,实际上你可以给每个文件单独设置.设置越多,可靠性越高,浪费空间多,写入效率降低.存储block的时候是可以分开存的,同时block的参数都可以设置(namenode上),但无论如何,写入都是一次性且单个写入头写入,eg:

- 我要写数据了:client->1->2->3,然后每个都写完了再报告上一级自己写完了:3->2->1->client.

### **基架感知**

它一定会把副本帮你相对分配到各个基架上,而不是塞到一个基架里.(鸡蛋不放进同一个篮子)当然这样做会提高写的代价.(网络距离变大了),哪怕replica超过4进入随机选基架模式,也不会roll到同一个基架上,它一定是分散开的(类似mongodb的mongos).

### **safemode**

namenode刚启动的时候(注意到之前提过它是第一个启动的),处于一种安全运行模态,等着底下的黑奴datanode发心跳和block-report,安全模式不对外提供访问,等到block的replica达到要求才退出安全模式.**简而言之就是刚开始的时候先封闭整个系统的对外访问,等到实际运行起来才开放**.

### **editlog**

可以简单类比为mysql里的log一样,记录你对文件系统的操作,先写日志,再实际操作.由于hdfs是在本地文件系统的上层,所以启动hdfs本身也需要被记录,这份记录就记录在本地文件系统里.至于其他的存fsimage文件里,这个也是在本地文件系统,这俩核心部件都不在hdfs里的.启动后会**先把上次可能的还没做完就退出的操作(但是记录在日志上的)先做了,然后确认hdfs状态和log状态一致后就可以删日志了(避免日志越来越长)** 上述的清理日志功能也可能由checkpoint来做.而上述的两个核心log也有副本,防止这两个核心部件出问题,且副本是**时时同步的**.

namenode和datanode中间的协议走的tcp/ip协议.任何⽂件都会有⼀个crc校验码, 避免传输损坏/恶意篡改.写pipeline, ⼀定要全部写完,读负载均衡.

机器上数量不平衡的时候会做一次再平衡操作.

为了保证数据的完整性,一般会生成checksum来校验.(单独文件存)
读写示意图:(实际的过程上述已经描述过了)
![读](C:/Users/77043/Desktop/应用/27-28/3.png)
![写](C:/Users/77043/Desktop/应用/27-28/2.png)







------

# 28-hbase

- 来源:hadop中的bigtable的开源实现.
- 存结构化数据和半结构化数据,没关系型数据库的结构要求这么高.本身的数据存储也可以分布式存储.
- 含有布隆过滤器帮助加速.
- 对第三方网关有支持.
- 支持auto sharding
- 支持谓词下推
- 支持map reduce

### **面向列存**

这导致hbase支持垂直分区,还有列组的概念.存的数据是有版本的.(建立在hdfs上)
行列交叉点称之为cell,cell是带版本的.可以通过时间戳或者版本向前追溯数据.在hbase里,key是严格有序的.

version(timestamp)还有⼀些时序保证,例如字段的时间戳倒序, 还有例如在t4时刻发出的数据请求, 在t6时刻被处理, 保证不会读到t5的数据(类似MVCC, 找第⼀个满⾜时序的version)的因果保证

列族支持动态加入到表里(同理,列也行),每个列都属于某个列族,所以这会导致可能不同行的列数不同,有些行不具备某些列.(**列族数目不建议超过三个!!!!**)因此是支持按列分区的(竖切),当然也支持横切,得到的产物被称为region,而region-server就和hdfs里的datanode基本一样了,而root就类似namenode,会记录元数据.

有时为了节省空间,当一段时间没变时,这个时间戳就不记了,比如t1,2,3,到t7才变,那可能中间的t5t6就没记下来.
**经典例子之webtable**
每⼀⾏是⼀个⽹⻚, 然后例如url这种数据和版本号匹配很好.

大致结构如下:
![](C:/Users/77043/Desktop/应用/27-28/4.png)

## transformer

论文结构如下,主要结构是编码器和解码器.
编码器接受输入并提取特征,解码器根据特征分析输出.
![](C:/Users/77043/Desktop/应用/27-28/6.png)
三个关键矩阵:查询矩阵,键矩阵,值矩阵:用于计算输入的特征之间的相关性.经过矩阵乘法,缩放,softmax,最后得到了注意力矩阵(组成编码器).一般有多个注意力矩阵.





------

# 29-Hive

Hadoop框架下的数据仓库，很多大厂还是使用hive进行存储

## 数据仓库与数据库的区别

### 1. 最大的区别是他是schema on read：Load一个data的时候不做任何校验，当发生查询的时候才会发生ETL操作

hive底层是HDFS，大量数据使用原始文件存储在HDFS，hive在上层读取它的内容，转化成同一个中间格式（parquet等），在上层使用类似sql的语句访问。

ETL操作：Extract, Transform, Load

**问题**：数据格式不是很好可能会有缺失，比如csv会有格子NULL，他转化为parquet会不会质量不行，什么时候做检查？

**回答**：LOAD DATA语句只会将文件加载进HDFS，只有当执行sql语句访问这个文件时才转化为parquet。转化需要的信息（如这个表格有哪些列）存储在metadata store里。

**schema on read优点**：数据会源源不断地进来但未必会使用，不要在加载的时候浪费大量的时间。数据湖也是类似。

mysql是schema on write，数据写入时进行校验，比如Not NULL列检查是否为NULL

## Running Hive

derby是内存数据库，存metadata
![img_25.png](C:/Users/77043/Desktop/应用/29-30/img_25.png)
<span style="color:red">省略了配置运行的教学内容</span>

## Hive partition and bucket

**partition**分区，Hive存储在分布式文件系统里，所以需要分区。和sql一样按照range或key分区

**bucket**是在分区的基础上再分区，1. 提高查询速度，2. 方便进行采样：
比如机器学习分割测试集，4个bucket抽80%训练集，希望抽出的数据符合整体数据的男女比例。
假如这时候bucket已经按性别划分了前两个为男性，后两个为女性，则每个bucket采样80%则满足抽样的比例

### parquet是一种列存

数据仓库经常做的是OLAP（在线分析处理），大多数时候针对一列处理，比如统计订单数量、销售总额

### parquet是一次性写入

不会在原文件上做修改。但是可以插入

## 存入sql不好吗，hive的优势在哪里

### 1. mysql必须是结构化表，hive可以是表也可以是csv,excel,txt

![img_26.png](C:/Users/77043/Desktop/应用/29-30/img_26.png)
图中：parquet、RCfile是两种不同的中间格式。意思就是hive会源文件->中间格式->表，在表上面进行sql操作。

## metastore

和hive server一起跑或者分开跑，两个hive server可以共用同一个metastore。

## 支持的file format以及很多都是列存

![img_27.png](C:/Users/77043/Desktop/应用/29-30/img_27.png)
标Column都是列存

### 1. 行存：每一行的数据在存储中是连续的

![img_28.png](C:/Users/77043/Desktop/应用/29-30/img_28.png)

### 2. HBase列族

![img_29.png](C:/Users/77043/Desktop/应用/29-30/img_29.png)
这还不是RCFile

### 3. RCFile列存

![img_30.png](C:/Users/77043/Desktop/应用/29-30/img_30.png)
row group之间是行存，row group里面是列存

### 4. ORCFile

![img_31.png](C:/Users/77043/Desktop/应用/29-30/img_31.png)
每个列如红笔标注依次存储，绿色记录每个column的起始offset

### parquet

类似的方式存储，但会做一些压缩。列之间的数据比较相似，可以采用类似事件序列数据的优化。

比如在重复数据较多时，可以采用bit vector来表示哪些行是某个数。需要权衡解压缩的时间。





------

# 30-flink

大互联网公司都在用的流式数据处理工具，处理有状态的事件流

## 流式数据处理的基本方法

在一个小的时间窗里执行批处理，当时间窗足够小的时候看起来像流式处理。
可以有边界，可以源源不断。可以实时，可以记录下来之后处理。需要支持有状态的处理，比如处理第3个数据需要用到处理前两个的结果。

## 状态怎么维护

内存或硬盘上，每个机器只能访问本地的。为了防止在内存或硬盘里丢失（为什么硬盘里会丢失），需要周期性增量性的快照。

### 如何做checkpoint

流里有一个栅栏事件，当栅栏事件进入operator，则会暂停当前流。当operator所有流栅栏都到达后，则记录状态。

**问题**：暂停会不会很浪费？
**回答**：在遇到一个栅栏时找另一个栅栏，并将未处理的部分，包括未输出的output buffer都记录在状态里。这样减少等待但是需要记录的状态更多
![img_39.png](C:/Users/77043/Desktop/应用/29-30/img_39.png)

checkpoint存储使用RocksDB

### 记录的状态有哪些

1. 当前事件流处理到了哪个事件。
2. 如果一个operator处理多个流，则要记录每个流到了哪个数据，即记录栅栏的位置。
3. 最终的结果(sink)
   ![img_38.png](C:/Users/77043/Desktop/应用/29-30/img_38.png)

## 时间是怎么描述的

所有事件必须打上时间戳。有一些不同的时间概念，事件产生的时间，处理时间

### flink time-related feature

![img_32.png](C:/Users/77043/Desktop/应用/29-30/img_32.png)

- 事件是什么时候产生的
- 水印事件，不代表任何语义动作，代表一个时间
- 处理的时候时间显得不一样，比如function同时处理两个event1，event2，分别有不同的t1，t2
- （无讲解）

### flink api

![img_33.png](C:/Users/77043/Desktop/应用/29-30/img_33.png)

- 关心业务可以用最高层的，类似hive采用sql，数据以表呈现但是是动态表，数据更改非常快
- 不是最高层，想控制流的处理过程
- 想把流放大，流是一系列事件构成的，我直接操作处理函数

<span style="color:red">代码演示内容只贴了代码</span>

#### 1. ProcessFunction api example

![img_34.png](C:/Users/77043/Desktop/应用/29-30/img_34.png)
![img_35.png](C:/Users/77043/Desktop/应用/29-30/img_35.png)

#### 2. DataStream api example

![img_36.png](C:/Users/77043/Desktop/应用/29-30/img_36.png)

#### 3. SQL

![img_37.png](C:/Users/77043/Desktop/应用/29-30/img_37.png)
点击事件30分钟分一组，最后得到每30分钟每个userId点击了几次

### watermark

![img_40.png](C:/Users/77043/Desktop/应用/29-30/img_40.png)
Source(1)处理过的时间是33，他只记录watermark时间，意味着这之前可能是乱序的，但33之前的我肯定是处理过的：即我之前可能处理过34，但是我不会没处理32。

同理Source(2)处理完的是17，map(1)处理完的是29，map(2)处理完的是17。

14和29同时到window(1)，则window处理完的只能是14。

### window

每固定时间作为一个时间窗：有可能有些时间窗很多，有些甚至没有

每固定个数的时间作为一个时间窗：遇到稀疏事件时，他的第一个事件就会显得不够实时

### architecture

![img_41.png](C:/Users/77043/Desktop/应用/29-30/img_41.png)
一个master多个worker，每个worker一个taskManager，一个taskManager多个task slot进程，一个task slot多个线程（spark、storm都是单一任务）

